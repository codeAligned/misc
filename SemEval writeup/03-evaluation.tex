\section{Analysis}
Our experiments with various pre-processors, different models, and an expanded training set had mixed results. We managed to establish fairly solid neutral and positive sentiment classifiers, with our LinearSVC bigram frequency model returning F1 scores of 73.79 and 68.98, respectively. However, the aim of the majority of our experiments was to improve upon our initially abysmal negative sentiment classification, and in this we met mild improvements and overall disappointment. 

\subsection{Naive Bayes}
The Naive Bayes classifier proved quite flexible, as we were able to increase the negative F1 score for MultinomialNB from 27.58 to 41.37 by implementing binary feature recognition (presence as opposed to frequency) and, more significantly, implementing trigrams. Comparing the Naive Bayes classifier assessed at the unigram level (overall 46.14) to the bigram (46.71) and trigram (52.12) reveals the large improvement n-grams can offer. This difference is even more notable when we consider that neutral and positive F1 scores hardly change from unigrams (69.09 and 65.78, respectively) to trigrams (63.61 and 65.78); rather, trigrams help the Naive Bayes classifier better recognize negative sentiments, increasing the negative F1 from 26.51 to 38.47 when using frequency counts. Tf-idf proved terrible at recognizing negative sentiments, with negative F1 scores of 0.17 and 0.00 at the bigram and trigram levels, respectively, despite otherwise comparable scores.

\subsection{Support Vector Machines}
The improvements caused by trigrams in the Naive Bayes classifier did not translate into our LinearSVC implementation. Rather, we found that implementing bigrams had a mixed effect and trigrams had a detrimental effect. When examining frequency, for example, the LinearSVC frequency unigram, bigram, and trigram negative F1 scores were 45.01, 42.45, and 36.62, respectively, dragging the overall score down from 56.73 to 52.42 despite neutral and positive F1 scores remaining stable. Binary feature recognition proved effective at the unigram level, but they only narrowly edged out frequency counts. Adding additional training data from the Sentiment140 lexicon also proved fruitless, as scores were unaffected or decreased, whether 5,000 or 50,000 extra training tweets were provided. 

\subsection{Logistic Regression}
We used a logistic regression algorithm provided by Scikit-learn in an attempt to see if a third statistical approach could help us achieve higher negative scores, but here again we were disappointed. Rather, logistic regression seems to perform on par with our higher-performing versions of LinearSVC, but lags still in negative sentiment identification. 